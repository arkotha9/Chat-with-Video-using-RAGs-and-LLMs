{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73263392",
   "metadata": {},
   "source": [
    "## Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfde37d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af92f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai langchain langchain_pinecone langchain[docarray] docarray pydantic==1.10.8 pytube python-dotenv tiktoken pinecone-client scikit-learn ruff git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d068454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "VIDEO = \"https://www.youtube.com/watch?v=BrsocJb-fAo&t=6\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f7254a",
   "metadata": {},
   "source": [
    "## Loading the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144bed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(openai_api_key = OPENAI_API_KEY, model = \"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ea9991",
   "metadata": {},
   "source": [
    "Now, when we use invoke function it returns an object of type *AIMessage*. We need to get a string result and here comes the StrOutputParser Class and the whole concept of langchain where chaining the objects togther to get a nicely formatted output can be seen.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5966050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chained_model = model | parser #chaining via the pipe operator\n",
    "chained_model.invoke(\"What is 2 + 2?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9fa855",
   "metadata": {},
   "source": [
    "## Prompt Template for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f9332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As gpt-3.5 is a chat based model, I ma using langchain's ChatPromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you cannnot answer the question by any means, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt.format(context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bf7e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chaining the prompt as well : prompt | model | parser\n",
    "chained_model = prompt | chained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d962dee",
   "metadata": {},
   "source": [
    "### Creating transcript for Youtube Video using pytube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf694112",
   "metadata": {},
   "source": [
    "Using pytube to download the youtube video. Of all the audio formats for the video (different bitrates) we choose the first one for now. We download the audio file into a temporary directory and only for the first time we are creating a transcription file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187a7893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import whisper\n",
    "from pytube import YouTube\n",
    "\n",
    "if not in os.path.exists(\"transcript.txt\"):\n",
    "    youtube = YouTube(VIDEO)\n",
    "    #choosing audio to transcribe\n",
    "    audio = youtube.streams.filter(only_audio=True).first()\n",
    "    whisper = whisper.load_model(\"base\")\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as tempdir:\n",
    "        file = audio.download(output_path = tempdir)\n",
    "        transcription = whisper_model.transcribe(file)[\"text\"].strip()\n",
    "        \n",
    "        with open(\"transcription.txt\", \"w\") as file:\n",
    "            file.write(transcription)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad08bb2d",
   "metadata": {},
   "source": [
    "### Splitting the transcription into documents\n",
    "Now we have the transcription which can be passed as the context. But the whole transcription cannot be passed as the models have a maximum context window length (~16000 tokens). So we split the transcription into chunks and retreive the relevant chunk to pass as context for a question. TO do that we use TextLoader from langchain and then split the Document instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed0ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_docs = TextLoader(\"transcription.txt\").load()\n",
    "\n",
    "#Using RecursiveCharecterTextSplitter, it splits the whole transcript into documents where each document has 1500 charecters with 50 charecters overlapping between two documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50)\n",
    "list_of_docs = text_splitter.split_documents(text_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7d07ba",
   "metadata": {},
   "source": [
    "### Retrieving the most relevant document for a question/query (R in RAG)\n",
    "The documents and the question are converted into embeddings which is basically a n-dimenional vector space for words/tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfdd6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe37b5e",
   "metadata": {},
   "source": [
    "Now we perform a similarity search between the question and the documents. The document closest to the question in the embedding space is returned as the relevant context to be passed into the prompt. Similarity metrics like cosine similiarity are generally used but I am going forward with storing in a vector database which handles all of that retrieval by itself.\n",
    "\n",
    "### Vector Databases\n",
    "- Can store large number of documents\n",
    "- Automatically creates and stores embeddings\n",
    "- Perform similarty search efficiently\n",
    "\n",
    "\n",
    "As vector database can help us retrieve the most relevant documents, all we need is a Retriever that can access the vector database and return the relevant context (most similar chunk to the question). This is done using RunnableParallel and RunnablePassThrough classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26369fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(list_of_docs, embeddings)\n",
    "retrieval_part = RunnableParallel(context = vectorstore.as_retriever(), question = RunnablePassThrough() )\n",
    "\n",
    "#chaining the retreiver to the pipeline\n",
    "final_model = retrieval_part | chained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c38591",
   "metadata": {},
   "source": [
    "### Augment and Generate Output\n",
    "We use the retrieved context and when we chain them using | operator, the prompt is *augmented* with the context and we use invoke function to *generate* the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can ask any question here\n",
    "final_model.invoke(\"What is Anime?\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a815147",
   "metadata": {},
   "source": [
    "### Replacing local vector DB with Pinecone \n",
    "Pinecone can handle large amounts of data and perform similarity searches at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca6a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "index_name = os.getenv(MY_INDEX) # my custom index name in pinecone from .env file\n",
    "\n",
    "pinecone = PineconeVectorStore.from_documents(\n",
    "    list_of_docs, embeddings, index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad557c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_from_pc = pinecone.as_retriever()\n",
    "\n",
    "final_model_1 = retrieval_from_pc | chained_model\n",
    "\n",
    "final_model_1.invoke(\"What is Hollywood going to start doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb078cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
