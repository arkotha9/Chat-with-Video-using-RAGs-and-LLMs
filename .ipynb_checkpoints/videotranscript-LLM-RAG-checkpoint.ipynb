{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73263392",
   "metadata": {},
   "source": [
    "## Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfde37d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openai"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\rojan\\anaconda3\\Lib\\site-packages\\pydantic-2.7.1.dist-info due to invalid metadata entry 'name'\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n",
      "python-lsp-black 1.2.1 requires black>=22.3.0, but you have black 0.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Obtaining dependency information for langchain-openai from https://files.pythonhosted.org/packages/1c/ff/d8bf3cacd55cabd85deed923a22a72e0c306a1211584f78a933512c3ef8f/langchain_openai-0.1.6-py3-none-any.whl.metadata\n",
      "  Using cached langchain_openai-0.1.6-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.46 (from langchain-openai)\n",
      "  Obtaining dependency information for langchain-core<0.2.0,>=0.1.46 from https://files.pythonhosted.org/packages/43/8b/48b7e6de9041d2b33d5108e154b82d1bd6c47cc68f0e44cb4fcdaccf5ec7/langchain_core-0.1.52-py3-none-any.whl.metadata\n",
      "  Using cached langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting openai<2.0.0,>=1.24.0 (from langchain-openai)\n",
      "  Obtaining dependency information for openai<2.0.0,>=1.24.0 from https://files.pythonhosted.org/packages/e7/a1/02370e280400237669fe1138490f289ccb8cc97f5ebf0bd1bdda1091fc53/openai-1.26.0-py3-none-any.whl.metadata\n",
      "  Using cached openai-1.26.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting tiktoken<1,>=0.5.2 (from langchain-openai)\n",
      "  Obtaining dependency information for tiktoken<1,>=0.5.2 from https://files.pythonhosted.org/packages/69/ca/0a71c1cdbf36da977bd306d295042087187954c32bfa259fa7afede0608b/tiktoken-0.6.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached tiktoken-0.6.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (6.0)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.2.0,>=0.1.46->langchain-openai)\n",
      "  Obtaining dependency information for jsonpatch<2.0,>=1.33 from https://files.pythonhosted.org/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl.metadata\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain-core<0.2.0,>=0.1.46->langchain-openai)\n",
      "  Obtaining dependency information for langsmith<0.2.0,>=0.1.0 from https://files.pythonhosted.org/packages/bb/dc/bdd085bde31e8246930e6bb34cb78b836501e18d503e9ec43cb0e337ce47/langsmith-0.1.55-py3-none-any.whl.metadata\n",
      "  Using cached langsmith-0.1.55-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.46->langchain-openai)\n",
      "  Obtaining dependency information for packaging<24.0,>=23.2 from https://files.pythonhosted.org/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl.metadata\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (2.7.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.46->langchain-openai) (8.2.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (3.5.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.24.0->langchain-openai)\n",
      "  Obtaining dependency information for distro<2,>=1.7.0 from https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl.metadata\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain-openai) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.46->langchain-openai) (2.1)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langchain-openai)\n",
      "  Obtaining dependency information for orjson<4.0.0,>=3.9.14 from https://files.pythonhosted.org/packages/f9/b7/3815984df03b677644c90cd4893d6293c80ef1c9f3a8493807bc1eb47da7/orjson-3.10.3-cp311-none-win_amd64.whl.metadata\n",
      "  Using cached orjson-3.10.3-cp311-none-win_amd64.whl.metadata (50 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langchain-openai) (2.18.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai) (1.26.16)\n",
      "Requirement already satisfied: colorama in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.24.0->langchain-openai) (0.4.6)\n",
      "Using cached langchain_openai-0.1.6-py3-none-any.whl (34 kB)\n",
      "Using cached langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
      "Using cached openai-1.26.0-py3-none-any.whl (314 kB)\n",
      "Using cached tiktoken-0.6.0-cp311-cp311-win_amd64.whl (798 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langsmith-0.1.55-py3-none-any.whl (120 kB)\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Using cached orjson-3.10.3-cp311-none-win_amd64.whl (138 kB)\n",
      "Installing collected packages: packaging, orjson, jsonpatch, distro, tiktoken, openai, langsmith, langchain-core, langchain-openai\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: jsonpatch\n",
      "    Found existing installation: jsonpatch 1.32\n",
      "    Uninstalling jsonpatch-1.32:\n",
      "      Successfully uninstalled jsonpatch-1.32\n",
      "Successfully installed distro-1.9.0 jsonpatch-1.33 langchain-core-0.1.52 langchain-openai-0.1.6 langsmith-0.1.55 openai-1.26.0 orjson-3.10.3 packaging-23.2 tiktoken-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af92f93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to c:\\users\\rojan\\appdata\\local\\temp\\pip-req-build-mckc48i4\n",
      "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: openai in c:\\users\\rojan\\anaconda3\\lib\\site-packages (1.26.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\rojan\\anaconda3\\lib\\site-packages (0.1.17)\n",
      "Requirement already satisfied: langchain_pinecone in c:\\users\\rojan\\anaconda3\\lib\\site-packages (0.1.0)\n",
      "Requirement already satisfied: docarray in c:\\users\\rojan\\anaconda3\\lib\\site-packages (0.32.1)\n",
      "Requirement already satisfied: pydantic==1.10.8 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (1.10.8)\n",
      "Requirement already satisfied: pytube in c:\\users\\rojan\\anaconda3\\lib\\site-packages (15.0.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\rojan\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\rojan\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: pinecone-client in c:\\users\\rojan\\anaconda3\\lib\\site-packages (3.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\rojan\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: ruff in c:\\users\\rojan\\anaconda3\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from pydantic==1.10.8) (4.8.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from langchain) (0.6.5)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.36 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from langchain) (0.0.37)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.48 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from langchain) (0.1.52)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from langchain) (0.1.55)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: orjson>=3.8.2 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from docarray) (3.10.3)\n",
      "Requirement already satisfied: rich>=13.1.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from docarray) (13.7.1)\n",
      "Requirement already satisfied: types-requests>=2.28.11.6 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from docarray) (2.31.0.20240406)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from docarray) (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from pinecone-client) (2023.7.22)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from pinecone-client) (2.2.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: numba in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from openai-whisper==20231117) (0.57.1)\n",
      "Requirement already satisfied: torch in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from openai-whisper==20231117) (2.2.1+cu118)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from openai-whisper==20231117) (8.12.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
      "Requirement already satisfied: hnswlib>=0.6.2 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from docarray) (0.8.0)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from docarray) (4.25.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.48->langchain) (23.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from rich>=13.1.0->docarray) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from rich>=13.1.0->docarray) (2.15.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from typing-inspect>=0.8.0->docarray) (1.0.0)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from numba->openai-whisper==20231117) (0.40.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\rojan\\appdata\\roaming\\python\\python311\\site-packages (from torch->openai-whisper==20231117) (3.13.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from torch->openai-whisper==20231117) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from torch->openai-whisper==20231117) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from torch->openai-whisper==20231117) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from torch->openai-whisper==20231117) (2023.4.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray) (0.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from jinja2->torch->openai-whisper==20231117) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\rojan\\anaconda3\\lib\\site-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git 'C:\\Users\\rojan\\AppData\\Local\\Temp\\pip-req-build-mckc48i4'\n"
     ]
    }
   ],
   "source": [
    "!pip install openai langchain langchain_pinecone langchain[docarray] docarray pydantic==1.10.8 pytube python-dotenv tiktoken pinecone-client scikit-learn ruff git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d068454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "VIDEO = \"https://www.youtube.com/watch?v=BrsocJb-fAo&t=6\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f7254a",
   "metadata": {},
   "source": [
    "## Loading the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "144bed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(openai_api_key = OPENAI_API_KEY, model = \"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ea9991",
   "metadata": {},
   "source": [
    "Now, when we use invoke function it returns an object of type *AIMessage*. We need to get a string result and here comes the StrOutputParser Class and the whole concept of langchain where chaining the objects togther to get a nicely formatted output can be seen.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5966050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chained_model = model | parser #chaining via the pipe operator\n",
    "chained_model.invoke(\"What is 2 + 2?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9fa855",
   "metadata": {},
   "source": [
    "## Prompt Template for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0f9332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As gpt-3.5 is a chat based model, I ma using langchain's ChatPromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you cannnot answer the question by any means, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt.format(context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93bf7e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chaining the prompt as well : prompt | model | parser\n",
    "chained_model = prompt | chained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d962dee",
   "metadata": {},
   "source": [
    "### Creating transcript for Youtube Video using pytube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf694112",
   "metadata": {},
   "source": [
    "Using pytube to download the youtube video. Of all the audio formats for the video (different bitrates) we choose the first one for now. We download the audio file into a temporary directory and only for the first time we are creating a transcription file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "187a7893",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (427698873.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[35], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    if not in os.path.exists(\"transcript.txt\"):\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import whisper\n",
    "from pytube import YouTube\n",
    "\n",
    "if not in os.path.exists(\"transcript.txt\"):\n",
    "    youtube = YouTube(VIDEO)\n",
    "    #choosing audio to transcribe\n",
    "    audio = youtube.streams.filter(only_audio=True).first()\n",
    "    whisper = whisper.load_model(\"base\")\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as tempdir:\n",
    "        file = audio.download(output_path = tempdir)\n",
    "        transcription = whisper_model.transcribe(file)[\"text\"].strip()\n",
    "        \n",
    "        with open(\"transcription.txt\", \"w\") as file:\n",
    "            file.write(transcription)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad08bb2d",
   "metadata": {},
   "source": [
    "### Splitting the transcription into documents\n",
    "Now we have the transcription which can be passed as the context. But the whole transcription cannot be passed as the models have a maximum context window length (~16000 tokens). So we split the transcription into chunks and retreive the relevant chunk to pass as context for a question. TO do that we use TextLoader from langchain and then split the Document instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ed0ec80",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error loading transcription.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py:42\u001b[0m, in \u001b[0;36mTextLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     43\u001b[0m         text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'transcription.txt'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextLoader\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[1;32m----> 4\u001b[0m text_docs \u001b[38;5;241m=\u001b[39m TextLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscription.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#Using RecursiveCharecterTextSplitter, it splits the whole transcript into documents where each document has 1500 charecters with 50 charecters overlapping between two documents\u001b[39;00m\n\u001b[0;32m      7\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1500\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:29\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy_load())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py:58\u001b[0m, in \u001b[0;36mTextLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     60\u001b[0m metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)}\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m Document(page_content\u001b[38;5;241m=\u001b[39mtext, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error loading transcription.txt"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_docs = TextLoader(\"transcription.txt\").load()\n",
    "\n",
    "#Using RecursiveCharecterTextSplitter, it splits the whole transcript into documents where each document has 1500 charecters with 50 charecters overlapping between two documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50)\n",
    "list_of_docs = text_splitter.split_documents(text_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7d07ba",
   "metadata": {},
   "source": [
    "### Retrieving the most relevant document for a question/query (R in RAG)\n",
    "The documents and the question are converted into embeddings which is basically a n-dimenional vector space for words/tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1cfdd6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe37b5e",
   "metadata": {},
   "source": [
    "Now we perform a similarity search between the question and the documents. The document closest to the question in the embedding space is returned as the relevant context to be passed into the prompt. Similarity metrics like cosine similiarity are generally used but I am going forward with storing in a vector database which handles all of that retrieval by itself.\n",
    "\n",
    "### Vector Databases\n",
    "- Can store large number of documents\n",
    "- Automatically creates and stores embeddings\n",
    "- Perform similarty search efficiently\n",
    "\n",
    "\n",
    "As vector database can help us retrieve the most relevant documents, all we need is a Retriever that can access the vector database and return the relevant context (most similar chunk to the question). This is done using RunnableParallel and RunnablePassThrough classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c26369fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_of_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocArrayInMemorySearch\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnableParallel, RunnablePassthrough\n\u001b[1;32m----> 4\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m DocArrayInMemorySearch(list_of_docs, embeddings)\n\u001b[0;32m      5\u001b[0m retrieval_part \u001b[38;5;241m=\u001b[39m RunnableParallel(context \u001b[38;5;241m=\u001b[39m vectorstore\u001b[38;5;241m.\u001b[39mas_retriever(), question \u001b[38;5;241m=\u001b[39m RunnablePassThrough() )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#chaining the retreiver to the pipeline\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'list_of_docs' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(list_of_docs, embeddings)\n",
    "retrieval_part = RunnableParallel(context = vectorstore.as_retriever(), question = RunnablePassThrough() )\n",
    "\n",
    "#chaining the retreiver to the pipeline\n",
    "final_model = retrieval_part | chained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c38591",
   "metadata": {},
   "source": [
    "### Augment and Generate Output\n",
    "We use the retrieved context and when we chain them using | operator, the prompt is *augmented* with the context and we use invoke function to *generate* the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can ask any question here\n",
    "final_model.invoke(\"What is Anime?\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a815147",
   "metadata": {},
   "source": [
    "### Replacing local vector DB with Pinecone \n",
    "Pinecone can handle large amounts of data and perform similarity searches at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fca6a0e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MY_INDEX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_pinecone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PineconeVectorStore\n\u001b[1;32m----> 3\u001b[0m index_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(MY_INDEX) \u001b[38;5;66;03m# my custom index name in pinecone from .env file\u001b[39;00m\n\u001b[0;32m      5\u001b[0m pinecone \u001b[38;5;241m=\u001b[39m PineconeVectorStore\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[0;32m      6\u001b[0m     list_of_docs, embeddings, index_name\u001b[38;5;241m=\u001b[39mindex_name\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MY_INDEX' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "index_name = os.getenv(MY_INDEX) # my custom index name in pinecone from .env file\n",
    "\n",
    "pinecone = PineconeVectorStore.from_documents(\n",
    "    list_of_docs, embeddings, index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad557c97",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pinecone' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m retrieval_from_pc \u001b[38;5;241m=\u001b[39m pinecone\u001b[38;5;241m.\u001b[39mas_retriever()\n\u001b[0;32m      3\u001b[0m final_model_1 \u001b[38;5;241m=\u001b[39m retrieval_from_pc \u001b[38;5;241m|\u001b[39m chained_model\n\u001b[0;32m      5\u001b[0m final_model_1\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is Hollywood going to start doing?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pinecone' is not defined"
     ]
    }
   ],
   "source": [
    "retrieval_from_pc = pinecone.as_retriever()\n",
    "\n",
    "final_model_1 = retrieval_from_pc | chained_model\n",
    "\n",
    "final_model_1.invoke(\"What is Hollywood going to start doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb078cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
